{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57c4a4ec",
   "metadata": {},
   "source": [
    "1. Zjistit jak jsem modifikoval\n",
    "2. Popisná satistika - sloupců\n",
    "3. Vzít si baseline model, vzít si skupinu modelů (klasifikace, regrese), Tunning, Report (výstupy metrik)\n",
    "4. Čas pro naučení\n",
    "5. Porovnání modelů (který např selže, provést analýzu na poruchových vzorcích, jaký je rozdíl mezi nimi)\n",
    "6. Umět to vysvětlit, rozumět tomu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c4081d",
   "metadata": {},
   "source": [
    "### Analýza datasetu\n",
    "\n",
    "Zobrazení kořenové struktury datasetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e67923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└── planets-dataset\n",
      "    ├── 3.complete\n",
      "    └── versions\n",
      "        └── 3\n",
      "            ├── planet\n",
      "            │   └── planet\n",
      "            │       ├── sample_submission.csv\n",
      "            │       ├── test-jpg\n",
      "            │       ├── train-jpg\n",
      "            │       └── train_classes.csv\n",
      "            └── test-jpg-additional\n",
      "                └── test-jpg-additional\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "space =  '    '; branch = '│   '; tee =    '├── '; last =   '└── '\n",
    "def tree(dir_path: Path, prefix: str='', exclude_extensions=None):\n",
    "    if exclude_extensions is None: exclude_extensions = ['.jpg']\n",
    "    if isinstance(dir_path, str): dir_path = Path(dir_path)\n",
    "    # Filter out files with excluded extensions\n",
    "    contents = [p for p in dir_path.iterdir()  if not (p.is_file() and p.suffix.lower() in exclude_extensions)]\n",
    "    if not contents: return\n",
    "    # contents each get pointers that are ├── with a final └── :\n",
    "    pointers = [tee] * (len(contents) - 1) + [last]\n",
    "    for pointer, path in zip(pointers, contents):\n",
    "        yield prefix + pointer + path.name\n",
    "        if path.is_dir():  # extend the prefix and recurse:\n",
    "            extension = branch if pointer == tee else space\n",
    "            # i.e. space because last, └── , above so no more |\n",
    "            yield from tree(path, prefix=prefix+extension, exclude_extensions=exclude_extensions)\n",
    "\n",
    "for line in tree('./nikitarom'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e914c94",
   "metadata": {},
   "source": [
    "První csv tabulka s názvem **sample_submission.csv** obsahuje 61191 hodnot, obsahuje 2 sloupce: image_name - název daného obrázku tags - popis vlastností toho co je na obrázcích.\n",
    "\n",
    "Druhá csv tabulka s názvem **train_classes.csv** obsahuje 40 479 hodnot obsahuje také 2 sloupce: image_name - název daného obrázku tags - popis vlastností toho co je na obrázcích např.: clear_primary, clear_cloudy_primary, atd...\n",
    "\n",
    "Máme adresáře pro obrázky:\n",
    "\n",
    "**test-jpg**, kde se nachází testovací obrázky, je jich přibližně 40 000.\n",
    "\n",
    "**train-jpg**, kde se nachází trénovací obrázky, je jich přibližně 40 000.\n",
    "\n",
    "**test-jpg-additional**, kde se nachází testovací ještě nachází přibližně 20 500 testovacích obrázků navíc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8fdcb",
   "metadata": {},
   "source": [
    "### Načtení dat pro trénovací model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import potřebných datasetů\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Nastavení cest k datům\n",
    "DATA_DIR = './nikitarom/planets-dataset/versions/3/'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'planet/planet/train-jpg')\n",
    "TEST_DIR = os.path.join(DATA_DIR, 'planet/planet/test-jpg')\n",
    "TRAIN_CLASSES = os.path.join(DATA_DIR, 'planet/planet/train_classes.csv')\n",
    "SUBMISSION = os.path.join(DATA_DIR, 'planet/planet/sample_submission.csv')\n",
    "\n",
    "# Načtení CSV souborů\n",
    "train_df = pd.read_csv(TRAIN_CLASSES)\n",
    "submission_df = pd.read_csv(SUBMISSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144edbaa",
   "metadata": {},
   "source": [
    "Naimportoval jsem potřebné datasety pro práci s daty. Určil jsem si cesty k adresářům s trénovací a testovacími obrázky a s popisem obrázků. Načetl jsem si csv soubory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c8d9e",
   "metadata": {},
   "source": [
    "#### Zjišťovaní informací o tabulkách"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd3041b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Počet trénovacích záznamů: 40479\n",
      "Počet testovacích záznamů: 61191\n",
      "\n",
      "Zobrazení informací o trénovací tabulce:\n",
      "Hlavička tabulky:    image_name                                       tags\n",
      "0    train_0                               haze primary\n",
      "1    train_1            agriculture clear primary water\n",
      "2    train_2                              clear primary\n",
      "3    train_3                              clear primary\n",
      "4    train_4  agriculture clear habitation primary road \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40479 entries, 0 to 40478\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   image_name  40479 non-null  object\n",
      " 1   tags        40479 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 632.6+ KB\n",
      "Informace o tabulce:  None \n",
      "\n",
      "Nulové hodnoty:  image_name    0\n",
      "tags          0\n",
      "dtype: int64 \n",
      "\n",
      "\n",
      "Počet unikátních tagů: 17\n",
      "Unikátní tagy: ['agriculture', 'artisinal_mine', 'bare_ground', 'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', 'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', 'road', 'selective_logging', 'slash_burn', 'water']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Počet trénovacích záznamů: {len(train_df)}\")\n",
    "print(f\"Počet testovacích záznamů: {len(submission_df)}\")\n",
    "\n",
    "# Zobrazení informací o tabulce\n",
    "print('\\nZobrazení informací o trénovací tabulce:')\n",
    "print(\"Hlavička tabulky: \", train_df.head(), '\\n')\n",
    "print(\"Informace o tabulce: \", train_df.info(), '\\n')\n",
    "print(\"Nulové hodnoty: \", train_df.isnull().sum(), '\\n')\n",
    "\n",
    "# Zobrazme si distribuci tagů ve sloupci 'tags'\n",
    "all_tags = []\n",
    "for tags in train_df['tags'].values:\n",
    "    all_tags.extend(tags.split())\n",
    "unique_tags = sorted(list(set(all_tags)))\n",
    "print(f\"\\nPočet unikátních tagů: {len(unique_tags)}\")\n",
    "print(f\"Unikátní tagy: {unique_tags}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da2016",
   "metadata": {},
   "source": [
    "Vypsal jsem si informace o tabulce, abych věděl kolik s ní je záznamů, jak má nastavené sloupce, jestli tam například jsou povoleny nulové hodnoty a jakého typu jsou. Také jsem si vypsal informace o tom, jestli obsahuje nějaké nulové hodnoty a kolik je tagů ve sloupci *tags*, kde jsem zjistil, že se vyskytuje 17 různých tagů pro popis obrázků.\n",
    "\n",
    "Unikátní tagy:\n",
    "\n",
    "agriculture, artisinal_mine, bare_ground, blooming, blow_down, clear, cloudy, conventional_mine, cultivation, habitation, haze, partly_cloudy, primary, road, selective_logging, slash_burn, water"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e47bb",
   "metadata": {},
   "source": [
    "#### Analýza frekvence tagů\n",
    "Abych měl přehled o tom kolikrát se tam který tag vyskytuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88e414f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nejčastější tagy:\n",
      "primary: 37513 výskytů\n",
      "clear: 28431 výskytů\n",
      "agriculture: 12315 výskytů\n",
      "road: 8071 výskytů\n",
      "water: 7411 výskytů\n",
      "partly_cloudy: 7261 výskytů\n",
      "cultivation: 4547 výskytů\n",
      "habitation: 3660 výskytů\n",
      "haze: 2697 výskytů\n",
      "cloudy: 2089 výskytů\n"
     ]
    }
   ],
   "source": [
    "tag_counts = {}\n",
    "for tag in all_tags:\n",
    "    if tag in tag_counts: tag_counts[tag] += 1\n",
    "    else: tag_counts[tag] = 1\n",
    "\n",
    "sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nNejčastější tagy:\")\n",
    "for tag, count in sorted_tags[:10]:\n",
    "    print(f\"{tag}: {count} výskytů\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91938924",
   "metadata": {},
   "source": [
    "### Trénování dat a vytvoření klasifikátorů"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b678d4c",
   "metadata": {},
   "source": [
    "#### Vytvoření one-hot encodingu pro tagy\n",
    "Abych mohl s tagy u obrázků trénovacím modelu pracovat, tak si musím udělat správný formát dat pro FloatTensor v PyTorch, kde je poté pomocí tensoru převedu do formátu, s kterým bude pracovat trénovací model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff19c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvoření one-hot encodingu pro tagy\n",
    "def get_tag_map(tags):\n",
    "    labels = np.zeros(len(unique_tags))\n",
    "    if pd.isna(tags): return labels\n",
    "    for tag in tags.split():\n",
    "        if tag in unique_tags:\n",
    "            labels[unique_tags.index(tag)] = 1\n",
    "    return labels\n",
    "# Přidání one-hot encodingu do dataframe\n",
    "train_df['tag_vector'] = train_df['tags'].apply(get_tag_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3650e37",
   "metadata": {},
   "source": [
    "#### Vytvoření dataset třídy pro PyTorch\n",
    "Abych mohl udělat trénovací model pro Resnet50, tak si musím vytvořit dataset třídu pro PyTorch. V této třídě si určím dataframe, s kterým budu pracovat, adresář pro obrázky a typ transformace pro dané obrázky. Je tu funkce, která mi vrací délku dataframu a funkce __getitem__, která načte obrázky, aplikuje na ně transformaci a pomocí PyTorch vytvoří tag_vector, který převede tagy do správného formátu k trénování datasetu a potom vrátíme vytvořený dataset pro zpracování s obrázky a tag vektorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740a071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvoření vlastní Dataset třídy pro PyTorch\n",
    "class PlanetDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.iloc[idx]['image_name']\n",
    "        img_path = os.path.join(self.img_dir, img_name + '.jpg')\n",
    "        # Načtení obrázku\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # Aplikace transformací, pokud jsou definované\n",
    "        if self.transform: image = self.transform(image)\n",
    "        # Získání one-hot encodingu pro tagy\n",
    "        tag_vector = torch.FloatTensor(self.dataframe.iloc[idx]['tag_vector'])\n",
    "        return image, tag_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef815c",
   "metadata": {},
   "source": [
    "#### Definování transformace\n",
    "Nadefinujem si transformaci pro trénovací a validační data, která bude použita pro trénování modelu. Můžem si nadefinovat zvlášť transformaci pro trénovací data a pro validační data.\n",
    "\n",
    "U transformace si můžeme určit.:\n",
    "- U jaké velikosti v pixelech chceme začít.\n",
    "- Jestli je převedeme na tensor.\n",
    "- Jestli chceme obrázek náhodně otáčet horizontálně nebo vertikálně. Tohle se hodně hodí pro obrázky tohoto typu, protože dané obrázky mohou být jakkoliv vyfoceny. \n",
    "- Náhodnou rotaci, u které si určíme počet stupňů.\n",
    "- Můžem si upravit obraz, jaký chceme mít jas, kontrast, sytost, a další...\n",
    "- Normalizaci, kde určíme průměr a směrodatnou odchylku na třídimenzionální data transformovaní obrázků.\n",
    "- A spoustu dalších vlastností..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a69c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definování transformací pro obrázky\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c247679",
   "metadata": {},
   "source": [
    "#### Rozdělení na trénovací data a jejich připracení pro trénování modelu\n",
    "Model si rozdělím na trénovací a validační data. Nastavím si rozdělení trénovacích dat a validačních dat 80/20 a random_state pro zamíchání dat. Vytvořím si datasety pro trénovací a validační data podle třídy PlanetDataset definované pro train model a poté si k datasetům vytvořím DataLoadery, ve kterých si určím batch size, který obvykle bývá 32, jestli je chci zamíchat a počet workerů. A potom si můžu vypsat ukázku načtení jedné části dat obrázků a labelů z train DataLoaderu. Vizualizuji si obrázky z datasetu. Na konci si ještě můžu nastavit hodnoty deformací obrázků pro průměr a smerodatnou odchylku, poté už jsou data připravena pro trénování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bfb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "print(f\"\\nPočet trénovacích vzorků: {len(train_data)}\")\n",
    "print(f\"Počet validačních vzorků: {len(valid_data)}\")\n",
    "# Vytvoření datasetů\n",
    "train_dataset = PlanetDataset(train_data, TRAIN_DIR, transform=train_transform)\n",
    "valid_dataset = PlanetDataset(valid_data, TRAIN_DIR, transform=val_transform)\n",
    "# Vytvoření dataloaderů\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "# Ukázka načtení jedné dávky dat\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"\\nTvar načtených obrázků: {images.shape}\")\n",
    "print(f\"Tvar načtených tagů: {labels.shape}\")\n",
    "# Vizualizace několika obrázků z datasetu\n",
    "def visualize_sample(dataset, num_samples=5):\n",
    "    plt.figure(figsize=(15, 3*num_samples))\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = image.permute(1, 2, 0).numpy()\n",
    "        # Denormalizace obrázku pro zobrazení\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "        tags = [unique_tags[j] for j in range(len(unique_tags)) if label[j] == 1]\n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(f\"Tagy: {', '.join(tags)}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# Hodnoty pro denormalizaci obrázků\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "# Zakomentujte následující řádek, pokud nechcete zobrazit ukázkové obrázky\n",
    "# visualize_sample(train_dataset)\n",
    "print(\"\\nData jsou připravena pro trénování modelu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11f144",
   "metadata": {},
   "source": [
    "#### Použití předtrénovaného modelu\n",
    "Vytvoříme si třídu multi-label klisifikátoru pro klasifikaci daných dat, aby se mohla trénovat. Když jsou data připravená, můžu už začít s předtrénovaným modelem resnet50. Zmrazí se prvních 10 vrstev. Nahradí finální plně připojenou vrstvu pro multi-label klasifikátor. Nastavíme si sekvenci, v jaký sekvenci chceme, aby nám trénovací model trénoval data. nadefinujem si nejdřív lineární transformaci pro plně připojenou vrstvu, poté ReLU, jaký chceme Dropout, poté tohleto ještě můžeme opakovat, jenom lineární transformace bude pro počet tříd. ještě máme udělanou funkci pro vrácení přední vrstvy klasifikátoru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a197c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "# vytvoření multi-label klasifikátoru pomocí ResNet50\n",
    "class PlanetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PlanetClassifier, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        # Zmrazení prvních 10 vrstev modelu\n",
    "        for param in list(self.resnet.parameters())[:-10]:\n",
    "            param.requires_grad = False\n",
    "        # Nahrazení finální plně připojené vrstvy pro multi-label klasifikaci\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3ad1c",
   "metadata": {},
   "source": [
    "#### Vytvoření trénovacího modelu\n",
    "Vytvoříme si funkci pro trénování modelu, kam nám bude vstupovat náš vytvořený klasifikátor, dataset loader trénovacích dat a dataset loader validačních dat, criterion zaznamenávající ztrátu, optimizer (nejčastější je Adam), scheduler (zaznamenává průběžný learning rate) a epochs (kolik chci provést epoch).\n",
    "\n",
    "Nejdříve si ve funkci určím device zařízení pro trénování modelu podle toho jestli chci cude (grafickou kartu od Nvidie) a jestli počítač nemá grafiku od Nvidie, tak se může zvolit cpu (procesor).\n",
    "\n",
    "Teďka je potřeba dosadit do našeho modelu toto zařízení, pomocí kterého se bude učit. Určíme si proměnnou pro zaznamenávání nejlepší hodnoty, podle které si potom na konci uložíme ten nejlepší model. \n",
    "\n",
    "Vytvoříme si seznamy pro ztráty tréningových dat a validačních dat, podle kterých poté budeme zobrazovat úspěšnost modelu.\n",
    "\n",
    "Model budeme učit v epochách (cyklech) tím, že si vytvoříme hlacní for-cyklus pro daný počet epoch, kolik chceme provést. Před dalším for cyklem vypíšeme na kolikáté epoše náš model je.\n",
    "\n",
    "**1. Trénování dat**\n",
    "Násleďně pomocí příkazu model.train() začne trénování v dané fázi modelu. Při tomto trénování si budem do proměnné running_loss zaznamenávat ztrátu. For-cyklem projdeme inputy a labely pro loader dataset trénovacích dat, kde je přidáme do paměti grafické karty nebo procesoru, který je bude učit. U optimizéru si nastavíme zero_grad(), nulový gradient. Inputy v modelu převedeme na outputy a násleďně vytvoříme proměnnou loss, kam uložíme podle kriterion, kam dáme outputy s labely. Uděláme loss.backward() ztrátových dat. Dále optimizer.step(). Poté uložíme do proměnné running_loss ztrátový item vynásobený velikostí inputu.\n",
    "\n",
    "Potom, když nám tento for-cyklus pro danou ecpochu proběhl můžeme udělat proměnnou epoch_train_loss, do které se uloží procentuální ztráta u každé epochy a tu následně uložíme do train_loses seznamu.\n",
    "\n",
    "**2.Validace dat**\n",
    "Teď je potřeba ještě projít validační data v dané fázi, kde začne vyhodnocování modelu pomocí příkazu model.eval(), u kterého zase budeme zaznamenávat running_loss ztrátu a bude mít seznamy pro ukládání predikátů a labelů. Pomocí PyTorch s vypnutím výpočtu gradientů s for-cyklem, který má inputy a labely pro loader dataset validačních dat provedeme validační fázi, která už je o něco jednodušší než u trénovacích dat. Zase inputy a labely přidáme do daného zařízení, uděláme outputy a vytvoříme loss proměnnou pomocí criterion, running_loss proměnnou, kde se zaznamená aktuální ztráta při validaci. u validačních dat vytvoříme binární predikce pomocí sigmoidy, kam budou vstupovat outputy a nastaví se u toho minimální dolní mez, odkud chceme, aby nám to bralo. Binární predikce ještě převedeme do float formátu a následně je uložíme do seznamu všech predikcí, které nám vrátí z CPU paměti. Do seznamu všech labelů také uložíme labely, které nám také vrátí z CPU paměti.\n",
    "\n",
    "Predikáty a labely z validačních dat převedeme do 1D dimenze a do formátu Numpy array. Do proměnné epoch_val_loss si uložíme úspěšnost validace.\n",
    "\n",
    "**Vyhodnocení dané fáze**\n",
    "Teď, když proběhlo trénování a validace, uděláme nějaké vyhodnocení dané fáze trénovacího modelu.\n",
    "\n",
    "Z daných dat uděláme precision score a recall score. Vypočítáme F1 score z dané fáze a potom celkové. Násleďně tyto data vypíšeme. Vypíšeme i F1 score pro nejlepší a nejhorší tagy.\n",
    "\n",
    "Aktualizujem learning rate pomocí schenduler. Nejlepší model uložíme do souboru typu pth. (Důležité)\n",
    "\n",
    "**Visualizace výsledků trénovacího modelu**\n",
    "Nakonec si uděláme graf vizualizace výsledků trénovacího modelu, který si uložíme do souboru a z celé funkce vrátíme model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3930813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Používám zařízení: {device}\")\n",
    "    model = model.to(device)\n",
    "    best_val_f1 = 0.0\n",
    "    train_losses = []; val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []; all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # Convert probabilities to binary predictions using 0.5 threshold\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                all_preds.append(preds.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        # Spojení všech dávek\n",
    "        all_preds = torch.cat(all_preds, dim=0).numpy()\n",
    "        all_labels = torch.cat(all_labels, dim=0).numpy()\n",
    "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        precision = precision_score(all_labels, all_preds, average='samples', zero_division=0)\n",
    "        recall = recall_score(all_labels, all_preds, average='samples', zero_division=0)\n",
    "        # F1 skóre pro každou třídu a průměr\n",
    "        sample_f1 = f1_score(all_labels, all_preds, average='samples', zero_division=0)\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        print(f'Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "        print(f'Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "        print(f'Sample F1: {sample_f1:.4f}, Macro F1: {macro_f1:.4f}')\n",
    "        # Výpis F1 skóre pro některé důležité tagy\n",
    "        tag_f1_scores = []\n",
    "        for i, tag in enumerate(unique_tags):\n",
    "            tag_f1 = f1_score(all_labels[:, i], all_preds[:, i], zero_division=0)\n",
    "            tag_f1_scores.append((tag, tag_f1))\n",
    "        # Seřazení tagů podle F1 skóre\n",
    "        tag_f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(\"\\nF1 skóre pro nejlepší tagy:\")\n",
    "        for tag, f1 in tag_f1_scores[:5]:\n",
    "            print(f\"{tag}: {f1:.4f}\")\n",
    "        print(\"\\nF1 skóre pro nejhorší tagy:\")\n",
    "        for tag, f1 in tag_f1_scores[-5:]:\n",
    "            print(f\"{tag}: {f1:.4f}\")\n",
    "        # Aktualizace learning rate\n",
    "        scheduler.step(epoch_val_loss)\n",
    "        # Uložení nejlepšího modelu\n",
    "        if sample_f1 > best_val_f1:\n",
    "            best_val_f1 = sample_f1\n",
    "            torch.save(model.state_dict(), 'best_planet_classifier.pth')\n",
    "            print(\"Uložen nejlepší model!\")\n",
    "    # Vizualizace výsledků tréninku\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Trénovací ztráta')\n",
    "    plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validační ztráta')\n",
    "    plt.xlabel('Epocha')\n",
    "    plt.ylabel('Ztráta')\n",
    "    plt.title('Trénovací a validační ztráta')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43204ba6",
   "metadata": {},
   "source": [
    "### Trénování modelu\n",
    "Nyní provedeme trénování modelu. Uděláme model z vytvořené vlastní třídy klasifikátoru. Určíme si criterion pro ztrátová data, určíme si optimizer a schenduler. Násleně všechny tyto proměnné dáme do funkce train model, která ám ho vytrénuje. Musí tam vstupovat klasifikátor modelu, daný loader trénovacích dat, loader validačních dat, criterion pro ztrátová data, optimizer, scheduler a počet fází (epoch) v kolika to chceme udělat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7251e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializace modelu, kritéria, optimizéru a scheduleru\n",
    "model = PlanetClassifier(num_classes=len(unique_tags))\n",
    "criterion = nn.BCEWithLogitsLoss()  # Vhodné pro multi-label klasifikaci\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n",
    "\n",
    "print(\"Trénování modelu...\")\n",
    "trained_model = train_model(model=model, train_loader=train_loader, val_loader=valid_loader, criterion=criterion, optimizer=optimizer, scheduler=scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb78079",
   "metadata": {},
   "source": [
    "### Vyhodnocení modelu\n",
    "Načteme si ten nejlepší model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_planet_classifier.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd3481",
   "metadata": {},
   "source": [
    "Funkce pro predikci a vytvoření submission souboru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74314c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, test_loader, submission_df):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []; image_names = []\n",
    "    with torch.no_grad():\n",
    "        for batch, (inputs, _) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float().cpu().numpy()\n",
    "            for i in range(len(preds)):\n",
    "                idx = batch * test_loader.batch_size + i\n",
    "                if idx < len(submission_df):\n",
    "                    img_name = submission_df.iloc[idx]['image_name']\n",
    "                    image_names.append(img_name)\n",
    "                    # Získání tagů pro predikci\n",
    "                    pred_tags = []\n",
    "                    for j, val in enumerate(preds[i]):\n",
    "                        if val == 1:\n",
    "                            pred_tags.append(unique_tags[j])\n",
    "                    predictions.append(' '.join(pred_tags))\n",
    "    # Vytvoření submission dataframe\n",
    "    submit_df = pd.DataFrame({'image_name': image_names, 'tags': predictions })\n",
    "    # Uložení do CSV\n",
    "    submit_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission soubor byl vytvořen!\")\n",
    "\n",
    "submission_df['tag_vector'] = [np.zeros(len(unique_tags)) for _ in range(len(submission_df))]\n",
    "\n",
    "test_dataset = PlanetDataset(submission_df, TEST_DIR, transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "create_submission(model, test_loader, submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
